{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a38a1b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (3.10.3)\n",
      "Requirement already satisfied: numpy in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (2.1.2)\n",
      "Requirement already satisfied: tensorboard in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (2.19.0)\n",
      "Requirement already satisfied: standard-imghdr in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (3.13.0)\n",
      "Collecting torchnet\n",
      "  Downloading torchnet-0.0.4.tar.gz (23 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (6.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (70.2.0)\n",
      "Requirement already satisfied: six>1.9 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: torch in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torchnet) (2.7.0+xpu)\n",
      "Collecting visdom (from torchnet)\n",
      "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m496.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: filelock in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torch->torchnet) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torch->torchnet) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torch->torchnet) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torch->torchnet) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torch->torchnet) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torch->torchnet) (2024.6.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt==2025.0.4 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torch->torchnet) (2025.0.4)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2025.0.4 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torch->torchnet) (2025.0.4)\n",
      "Requirement already satisfied: intel-cmplr-lic-rt==2025.0.4 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torch->torchnet) (2025.0.4)\n",
      "Requirement already satisfied: intel-sycl-rt==2025.0.4 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torch->torchnet) (2025.0.4)\n",
      "Requirement already satisfied: tcmlib==1.2.0 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torch->torchnet) (1.2.0)\n",
      "Requirement already satisfied: umf==0.9.1 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torch->torchnet) (0.9.1)\n",
      "Requirement already satisfied: intel-pti==0.10.1 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torch->torchnet) (0.10.1)\n",
      "Requirement already satisfied: pytorch-triton-xpu==3.3.0 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from torch->torchnet) (3.3.0)\n",
      "Requirement already satisfied: scipy in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from visdom->torchnet) (1.15.3)\n",
      "Collecting requests (from visdom->torchnet)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tornado in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from visdom->torchnet) (6.5)\n",
      "Collecting jsonpatch (from visdom->torchnet)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting websocket-client (from visdom->torchnet)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from sympy>=1.13.3->torch->torchnet) (1.3.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch->visdom->torchnet)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->visdom->torchnet)\n",
      "  Using cached charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->visdom->torchnet)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->visdom->torchnet)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->visdom->torchnet)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Building wheels for collected packages: torchnet, visdom\n",
      "  Building wheel for torchnet (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchnet: filename=torchnet-0.0.4-py3-none-any.whl size=29637 sha256=dd903e0d78dbaf8a90ec059822837eb7f9d22292b46553a8a2a377823df3b4bf\n",
      "  Stored in directory: /home/teogalletta/.cache/pip/wheels/a6/bc/4e/f549d68d5cbbf63fcff76ed29d5c870bc6d66f177dbcaa65a2\n",
      "  Building wheel for visdom (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1404372 sha256=8c16e3cc6add482ed256fe0d160d1f84b9191fa09b75e0dcd9c1c4018cf1f201\n",
      "  Stored in directory: /home/teogalletta/.cache/pip/wheels/b6/41/00/ddecc6975408e60d3ffa9b5de9993a941bb050804d2066ac2b\n",
      "Successfully built torchnet visdom\n",
      "Installing collected packages: websocket-client, urllib3, jsonpointer, idna, charset-normalizer, certifi, requests, jsonpatch, visdom, torchnet\n",
      "Successfully installed certifi-2025.4.26 charset-normalizer-3.4.2 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 requests-2.32.3 torchnet-0.0.4 urllib3-2.4.0 visdom-0.2.4 websocket-client-1.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib numpy tensorboard standard-imghdr torchnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0534afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from ImageToStringNet import ImageToStringNet, classes\n",
    "\n",
    "from torchnet.meter import AverageValueMeter\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd7e5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  96\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes: \", len(classes))\n",
    "\n",
    "class DigitDataset(Dataset):\n",
    "    def __init__(self, imgs_path, txt_path, transform=None):\n",
    "        self.imgs_path = imgs_path\n",
    "        self.images = np.loadtxt(txt_path, dtype=str, delimiter='\\t', comments=[])\n",
    "        self.transform = transform\n",
    "        self.targets = [classes.index(c) for _, c, _, _ in self.images]\n",
    "        self.targets = torch.LongTensor(self.targets)\n",
    "        \n",
    "        self.data = [Image.open(f'{self.imgs_path}/{i}').convert('L') for i, _, _, _ in self.images]\n",
    "        self.data = [self.transform(i) for i in self.data]\n",
    "        self.data = torch.stack(self.data, dim=0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _, _, t, b = self.images[index]\n",
    "\n",
    "        # Image Tensor, Label, Top Margin (%), Bottom Margin (%)\n",
    "        return self.data[index], self.targets[index], np.float32(t), np.float32(b)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "244fcb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "dataset_dir = \"../dataset/digit_dataset\"\n",
    "trainset = DigitDataset(imgs_path=dataset_dir, txt_path=\"../dataset/digit_dataset/train.txt\", transform=transforms.ToTensor())\n",
    "testset = DigitDataset(imgs_path=dataset_dir, txt_path=\"../dataset/digit_dataset/test.txt\", transform=transforms.ToTensor())\n",
    "\n",
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "# helper function to show an image\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d35060c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ImageToStringNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b1ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e73c98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/exp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8988577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "next(dataiter)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0402b84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels, t, b = next(dataiter)\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_img-to-str_images', img_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61a3f94",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e677528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net, (images, t, b))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf20f8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape:  torch.Size([100, 1, 28, 28])\n",
      "Labels shape:  torch.Size([100])\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "print(\"Images shape: \", images.shape)\n",
    "print(\"Labels shape: \", labels.shape)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "521483c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def images_to_probs(net, images, top_margin, bottom_margin):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images, top_margin, bottom_margin)\n",
    "    # convert output probabilities to predicted class\n",
    "\n",
    "    output = output.cpu()\n",
    "\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels, top_margin, bottom_margin):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images, top_margin, bottom_margin)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "\n",
    "    images = images.cpu()\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "498e76f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.xpu.is_available():\n",
    "    net = net.xpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13499442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 0.074 accuracy: 0.219\n",
      "[1,  2000] loss: 0.072 accuracy: 0.220\n",
      "[1,  3000] loss: 0.070 accuracy: 0.221\n",
      "[1,  4000] loss: 0.068 accuracy: 0.222\n",
      "[1,  5000] loss: 0.066 accuracy: 0.222\n",
      "[1,  6000] loss: 0.065 accuracy: 0.223\n",
      "[1,  7000] loss: 0.063 accuracy: 0.224\n",
      "[1,  8000] loss: 0.061 accuracy: 0.224\n",
      "[1,  9000] loss: 0.061 accuracy: 0.225\n",
      "[1, 10000] loss: 0.060 accuracy: 0.225\n",
      "[1, 11000] loss: 0.060 accuracy: 0.225\n",
      "[1, 12000] loss: 0.060 accuracy: 0.225\n",
      "[1, 13000] loss: 0.059 accuracy: 0.226\n",
      "[1, 14000] loss: 0.058 accuracy: 0.226\n",
      "[1, 15000] loss: 0.057 accuracy: 0.226\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \t\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \toptimizer.step()\n\u001b[32m     22\u001b[39m \toptimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "loss_meter = AverageValueMeter()\n",
    "accuracy_meter = AverageValueMeter()\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\tfor mode in [\"train\", \"test\"]:\n",
    "\t\tloss_meter.reset()\n",
    "\t\taccuracy_meter.reset()\n",
    "\n",
    "\t\tfor i, data in enumerate(trainloader, 0):\n",
    "\t\t\twith torch.set_grad_enabled(mode == \"train\"):\n",
    "\t\t\t\t# get the inputs; data is a list of [inputs, labels]\n",
    "\t\t\t\tinputs, labels, t, b = data\n",
    "\t\t\t\tinputs, labels, t, b = inputs.xpu(), labels.xpu(), t.xpu(), b.xpu()\n",
    "\n",
    "\t\t\t\t# forward + backward + optimize\n",
    "\t\t\t\toutputs = net(inputs, t, b)\n",
    "\t\t\t\tloss = criterion(outputs, labels)\n",
    "\n",
    "\t\t\t\tif mode == \"train\":\n",
    "\t\t\t\t\tloss.backward()\n",
    "\t\t\t\t\toptimizer.step()\n",
    "\t\t\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t\taccuracy_meter.add(accuracy_score(labels.cpu(), outputs.argmax(dim=1).cpu()), n=inputs.size(0))\n",
    "\t\t\t\tloss_meter.add(loss.item(), n=inputs.size(0))\n",
    "\n",
    "\t\t\t\tif i % 1000 == 999:    # every 1000 mini-batches...\n",
    "\t\t\t\t\tprint(f'[{epoch + 1}, {i + 1:5d}] loss: {loss_meter.value()[0]:.3f} accuracy: {accuracy_meter.value()[0]:.3f}')\n",
    "\t\t\t\t\tif mode == \"train\":\n",
    "\t\t\t\t\t# ...log the running loss\n",
    "\t\t\t\t\t\twriter.add_scalar('loss/train',\n",
    "\t\t\t\t\t\t\t\t\t\tloss_meter.value()[0],\n",
    "\t\t\t\t\t\t\t\t\t\tepoch * len(trainloader) + i)\n",
    "\t\t\t\t\t\twriter.add_scalar('accuracy/train',\n",
    "\t\t\t\t\t\t\t\t\t\taccuracy_meter.value()[0],\n",
    "\t\t\t\t\t\t\t\t\t\tepoch * len(trainloader) + i)\n",
    "\n",
    "\t\t\t\t\t# ...log a Matplotlib Figure showing the model's predictions on a\n",
    "\t\t\t\t\t# random mini-batch\n",
    "\t\t\t\t\t\twriter.add_figure('predictions vs. actuals',\n",
    "\t\t\t\t\t\t\t\t\t\tplot_classes_preds(net, inputs, labels, t, b),\n",
    "\t\t\t\t\t\t\t\t\t\tglobal_step=epoch * len(trainloader) + i)\n",
    "\t\t\n",
    "\t\twriter.add_scalar(f'loss/{mode}', loss_meter.value()[0], epoch)\n",
    "\t\twriter.add_scalar(f'accuracy/{mode}', accuracy_meter.value()[0], epoch)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0a2e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. gets the probability predictions in a test_size x num_classes Tensor\n",
    "# 2. gets the preds in a test_size Tensor\n",
    "# takes ~10 seconds to run\n",
    "class_probs = []\n",
    "class_label = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels, t, b = data\n",
    "\n",
    "        if torch.xpu.is_available():\n",
    "            images, t, b = images.xpu(), t.xpu(), b.xpu()\n",
    "\n",
    "        output = net(images, t, b)\n",
    "\n",
    "        output = output.cpu()\n",
    "\n",
    "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_label.append(labels)\n",
    "\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_label = torch.cat(class_label)\n",
    "\n",
    "# helper function\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "    tensorboard_truth = test_label == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_truth,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    writer.close()\n",
    "\n",
    "# plot all the pr curves\n",
    "for i in range(len(classes)):\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2acf64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657439bc",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5ad3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Intel GPU (XPU)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    print(\"Using Intel GPU (XPU)\")\n",
    "else:\n",
    "    print(\"No supported GPU backend detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb8dc617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "conv1 = nn.Conv2d(1, 6, 5)\n",
    "pool = nn.MaxPool2d(2, 2)\n",
    "conv2 = nn.Conv2d(6, 16, 5)\n",
    "fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "fc2 = nn.Linear(120, 84)\n",
    "fc3 = nn.Linear(84, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "621042a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DigitDataset at 0x7f7504933620>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc3bcb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: torch.Size([6, 24, 24])\n",
      "F.relu: torch.Size([6, 24, 24])\n",
      "pool: torch.Size([6, 12, 12])\n",
      "conv2: torch.Size([16, 8, 8])\n",
      "F.relu: torch.Size([16, 8, 8])\n",
      "pool: torch.Size([16, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.rand(1,28,28)\n",
    "x = conv1(img)\n",
    "print(\"conv1:\",x.shape)\n",
    "\n",
    "x = F.relu(x)\n",
    "print(\"F.relu:\",x.shape)\n",
    "\n",
    "x = pool(x)\n",
    "print(\"pool:\",x.shape)\n",
    "\n",
    "x = conv2(x)\n",
    "print(\"conv2:\",x.shape)\n",
    "\n",
    "x = F.relu(x)\n",
    "print(\"F.relu:\",x.shape)\n",
    "\n",
    "x = pool(x)\n",
    "print(\"pool:\",x.shape)\n",
    "\n",
    "x.view(-1, 16 * 4 * 4).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
