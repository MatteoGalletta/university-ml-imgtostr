{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a38a1b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (3.10.3)\n",
      "Requirement already satisfied: numpy in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (2.1.2)\n",
      "Requirement already satisfied: tensorboard in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (2.19.0)\n",
      "Collecting standard-imghdr\n",
      "  Downloading standard_imghdr-3.13.0-py3-none-any.whl.metadata (862 bytes)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (6.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (70.2.0)\n",
      "Requirement already satisfied: six>1.9 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/teogalletta/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Downloading standard_imghdr-3.13.0-py3-none-any.whl (4.6 kB)\n",
      "Installing collected packages: standard-imghdr\n",
      "Successfully installed standard-imghdr-3.13.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib numpy tensorboard standard-imghdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0534afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image, ImageDraw, ImageFont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd7e5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  96\n"
     ]
    }
   ],
   "source": [
    "classes = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.:!?'()[]{}<>/\\\\@#$€£%&~aèéìòù-+°\"\n",
    "print(\"Classes: \", len(classes))\n",
    "\n",
    "class DigitDataset(Dataset):\n",
    "    def __init__(self, imgs_path, txt_path, transform=None):\n",
    "        self.imgs_path = imgs_path\n",
    "        self.images = np.loadtxt(txt_path, dtype=str, delimiter='\\t', comments=[])\n",
    "        self.transform = transform\n",
    "        self.targets = [classes.index(c) for _, c, _, _ in self.images]\n",
    "        self.targets = torch.LongTensor(self.targets)\n",
    "        \n",
    "        self.data = [Image.open(f'{self.imgs_path}/{i}').convert('L') for i, _, _, _ in self.images]\n",
    "        self.data = [self.transform(i) for i in self.data]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _, _, t, b = self.images[index]\n",
    "\n",
    "        # Image Tensor, Label, Top Margin (%), Bottom Margin (%)\n",
    "        return self.data[index], self.targets[index], np.float32(t), np.float32(b)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "244fcb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "dataset_dir = \"../dataset/digit_dataset\"\n",
    "trainset = DigitDataset(imgs_path=dataset_dir, txt_path=\"../dataset/digit_dataset/train.txt\", transform=transforms.ToTensor())\n",
    "testset = DigitDataset(imgs_path=dataset_dir, txt_path=\"../dataset/digit_dataset/test.txt\", transform=transforms.ToTensor())\n",
    "\n",
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "# constant for classes\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# helper function to show an image\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d35060c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4 + 2, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, len(classes))\n",
    "\n",
    "    def forward(self, x, top_margin, bottom_margin):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        \n",
    "        x = torch.cat((x, top_margin.view(-1, 1)), dim=1)\n",
    "        x = torch.cat((x, bottom_margin.view(-1, 1)), dim=1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "75b1ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e73c98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/exp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a8988577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " tensor([89, 60, 29, 48]),\n",
       " tensor([0.0800, 0.0741, 0.1923, 0.2800]),\n",
       " tensor([0.3200, 0.2593, 0.1923, 0.0000])]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0402b84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels, t, b = next(dataiter)\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_img-to-str_images', img_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61a3f94",
   "metadata": {},
   "source": [
    "tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e677528f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Net.forward() missing 2 required positional arguments: 'top_margin' and 'bottom_margin'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[214]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m writer.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages/torch/utils/tensorboard/writer.py:841\u001b[39m, in \u001b[36mSummaryWriter.add_graph\u001b[39m\u001b[34m(self, model, input_to_model, verbose, use_strict_trace)\u001b[39m\n\u001b[32m    838\u001b[39m torch._C._log_api_usage_once(\u001b[33m\"\u001b[39m\u001b[33mtensorboard.logging.add_graph\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    839\u001b[39m \u001b[38;5;66;03m# A valid PyTorch model should have a 'forward' method\u001b[39;00m\n\u001b[32m    840\u001b[39m \u001b[38;5;28mself\u001b[39m._get_file_writer().add_graph(\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[43mgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_to_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_strict_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages/torch/utils/tensorboard/_pytorch_graph.py:326\u001b[39m, in \u001b[36mgraph\u001b[39m\u001b[34m(model, args, verbose, use_strict_trace)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _set_model_to_eval(model):\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         trace = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_strict_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m         graph = trace.graph\n\u001b[32m    328\u001b[39m         torch._C._jit_pass_inline(graph)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages/torch/jit/_trace.py:1002\u001b[39m, in \u001b[36mtrace\u001b[39m\u001b[34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[39m\n\u001b[32m    989\u001b[39m     warnings.warn(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`optimize` is deprecated and has no effect. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse `with torch.jit.optimized_execution()` instead\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    992\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    993\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    994\u001b[39m     )\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    997\u001b[39m     check_if_torch_exportable,\n\u001b[32m    998\u001b[39m     log_torch_jit_trace_exportability,\n\u001b[32m    999\u001b[39m     log_torchscript_usage,\n\u001b[32m   1000\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m traced_func = \u001b[43m_trace_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_compilation_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m log_torchscript_usage(\u001b[33m\"\u001b[39m\u001b[33mtrace\u001b[39m\u001b[33m\"\u001b[39m, model_id=_get_model_id(traced_func))\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_if_torch_exportable():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages/torch/jit/_trace.py:696\u001b[39m, in \u001b[36m_trace_impl\u001b[39m\u001b[34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[39m\n\u001b[32m    694\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    695\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mexample_kwarg_inputs should be a dict\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforward\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    710\u001b[39m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[33m\"\u001b[39m\u001b[33m__self__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    711\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func.\u001b[34m__self__\u001b[39m, torch.nn.Module)\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    713\u001b[39m ):\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages/torch/jit/_trace.py:1279\u001b[39m, in \u001b[36mtrace_module\u001b[39m\u001b[34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[39m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1278\u001b[39m     example_inputs = make_tuple(example_inputs)\n\u001b[32m-> \u001b[39m\u001b[32m1279\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_c\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1290\u001b[39m check_trace_method = module._c._get_method(method_name)\n\u001b[32m   1292\u001b[39m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1741\u001b[39m, in \u001b[36mModule._slow_forward\u001b[39m\u001b[34m(self, *input, **kwargs)\u001b[39m\n\u001b[32m   1739\u001b[39m         recording_scopes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1740\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1741\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1743\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "\u001b[31mTypeError\u001b[39m: Net.forward() missing 2 required positional arguments: 'top_margin' and 'bottom_margin'"
     ]
    }
   ],
   "source": [
    "writer.add_graph(net, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "cf20f8d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[215]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data[perm][:n], labels[perm][:n]\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# select random images and their target indices\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m images, labels, t, b = \u001b[43mselect_n_random\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# get the class labels for each image\u001b[39;00m\n\u001b[32m     15\u001b[39m class_labels = [classes[lab] \u001b[38;5;28;01mfor\u001b[39;00m lab \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[215]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mselect_n_random\u001b[39m\u001b[34m(data, labels, n)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) == \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[32m      8\u001b[39m perm = torch.randperm(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mperm\u001b[49m\u001b[43m]\u001b[49m[:n], labels[perm][:n]\n",
      "\u001b[31mTypeError\u001b[39m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels, t, b = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521483c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def images_to_probs(net, images, top_margin, bottom_margin):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images, top_margin, bottom_margin)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels, top_margin, bottom_margin):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images, top_margin, bottom_margin)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13499442",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Net.forward() missing 2 required positional arguments: 'top_margin' and 'bottom_margin'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[217]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     22\u001b[39m             writer.add_scalar(\u001b[33m'\u001b[39m\u001b[33mtraining loss\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     23\u001b[39m                             running_loss / \u001b[32m1000\u001b[39m,\n\u001b[32m     24\u001b[39m                             epoch * \u001b[38;5;28mlen\u001b[39m(trainloader) + i)\n\u001b[32m     26\u001b[39m             \u001b[38;5;66;03m# ...log a Matplotlib Figure showing the model's predictions on a\u001b[39;00m\n\u001b[32m     27\u001b[39m             \u001b[38;5;66;03m# random mini-batch\u001b[39;00m\n\u001b[32m     28\u001b[39m             writer.add_figure(\u001b[33m'\u001b[39m\u001b[33mpredictions vs. actuals\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m                             \u001b[43mplot_classes_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     30\u001b[39m                             global_step=epoch * \u001b[38;5;28mlen\u001b[39m(trainloader) + i)\n\u001b[32m     31\u001b[39m             running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFinished Training\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[216]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mplot_classes_preds\u001b[39m\u001b[34m(net, images, labels)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_classes_preds\u001b[39m(net, images, labels):\n\u001b[32m     16\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    Generates matplotlib Figure using a trained network, along with images\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m    and labels from a batch, that shows the network's top prediction along\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[33;03m    Uses the \"images_to_probs\" function.\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     preds, probs = \u001b[43mimages_to_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# plot the images in the batch, along with predicted and true labels\u001b[39;00m\n\u001b[32m     25\u001b[39m     fig = plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m48\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[216]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mimages_to_probs\u001b[39m\u001b[34m(net, images)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimages_to_probs\u001b[39m(net, images):\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    Generates predictions and corresponding probabilities from a trained\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    network and a list of images\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     output = \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# convert output probabilities to predicted class\u001b[39;00m\n\u001b[32m     10\u001b[39m     _, preds_tensor = torch.max(output, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/university-ml-imgtostr/.venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: Net.forward() missing 2 required positional arguments: 'top_margin' and 'bottom_margin'"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels, t, b = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs, t, b)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # every 1000 mini-batches...\n",
    "\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 1000,\n",
    "                            epoch * len(trainloader) + i)\n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "            writer.add_figure('predictions vs. actuals',\n",
    "                            plot_classes_preds(net, inputs, labels, t, b),\n",
    "                            global_step=epoch * len(trainloader) + i)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. gets the probability predictions in a test_size x num_classes Tensor\n",
    "# 2. gets the preds in a test_size Tensor\n",
    "# takes ~10 seconds to run\n",
    "class_probs = []\n",
    "class_label = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        output = net(images)\n",
    "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_label.append(labels)\n",
    "\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_label = torch.cat(class_label)\n",
    "\n",
    "# helper function\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "    tensorboard_truth = test_label == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_truth,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    writer.close()\n",
    "\n",
    "# plot all the pr curves\n",
    "for i in range(len(classes)):\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657439bc",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Intel GPU (XPU)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    print(\"Using Intel GPU (XPU)\")\n",
    "else:\n",
    "    print(\"No supported GPU backend detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb8dc617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "conv1 = nn.Conv2d(1, 6, 5)\n",
    "pool = nn.MaxPool2d(2, 2)\n",
    "conv2 = nn.Conv2d(6, 16, 5)\n",
    "fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "fc2 = nn.Linear(120, 84)\n",
    "fc3 = nn.Linear(84, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "621042a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc3bcb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: torch.Size([6, 24, 24])\n",
      "F.relu: torch.Size([6, 24, 24])\n",
      "pool: torch.Size([6, 12, 12])\n",
      "conv2: torch.Size([16, 8, 8])\n",
      "F.relu: torch.Size([16, 8, 8])\n",
      "pool: torch.Size([16, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.rand(1,28,28)\n",
    "x = conv1(img)\n",
    "print(\"conv1:\",x.shape)\n",
    "\n",
    "x = F.relu(x)\n",
    "print(\"F.relu:\",x.shape)\n",
    "\n",
    "x = pool(x)\n",
    "print(\"pool:\",x.shape)\n",
    "\n",
    "x = conv2(x)\n",
    "print(\"conv2:\",x.shape)\n",
    "\n",
    "x = F.relu(x)\n",
    "print(\"F.relu:\",x.shape)\n",
    "\n",
    "x = pool(x)\n",
    "print(\"pool:\",x.shape)\n",
    "\n",
    "x.view(-1, 16 * 4 * 4).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
