\chapter{Codice}
\begin{figure}[H]
    \centering
    \tikzstyle{every node}=[draw=black, thick, anchor=west]
    \tikzstyle{selected}=[draw=red, fill=red!30]
    \tikzstyle{optional}=[dashed, fill=gray!50]

    \begin{tikzpicture}[%
      grow via three points={one child at (0.5,-0.68) and
                             two children at (0.5,-0.71) and (0.5,-1.45)},
      edge from parent path={(\tikzparentnode.south) |- (\tikzchildnode.west)}]

      \node {university-ml-imgtostr/}
        child { node {\textbf{core}/}
            child { node {ImageToStringClassifier.py} }
            child { node {ImageToStringPostprocessing.py} }
            child { node {ImageToStringPreprocessing.py} }
        }
        child [missing] {}
        child [missing] {}
        child [missing] {}
        child { node {\textbf{dataset}/}
            child { node {dataset.ipynb} }
                child { node {\textbf{digit\_dataset/}} }
                child { node {\textbf{english\_words\_dataset/}} }
                child { node {\textbf{words\_dataset/}} }
        }
        child [missing] {}
        child [missing] {}
        child [missing] {}
        child [missing] {}
        child { node {\textbf{demo}/}
            child { node {demo.py} }
        }
        child [missing] {}
        child { node {\textbf{src}/}
            child { node {evaluation\_english\_words.ipynb} }
            child { node {evaluation\_random\_words.ipynb} }
            child { node {ImageToStringNet.py} }
            child { node {main.ipynb} }
        }
        child [missing] {}
        child [missing] {}
        child [missing] {}
        child [missing] {};

    \end{tikzpicture}
    \caption{Struttura progetto}
\end{figure}

\section{Core}
La cartella \emph{core} contiene il codice delle tre classi dedicate a preprocessing, classificazione e postprocessing. Ciascun file definisce una classe omonima.

\subsection*{PreProcessing}
La classe \texttt{ImageToStringPreprocessing} prepara l'immagine per la fase successiva di classificazione, segmentando le lettere e normalizzandole in un formato uniforme. A partire da un'immagine in input, esegue operazioni come conversione in scala di grigi, binarizzazione e inversione del contrasto se necessario. 
Successivamente, rileva e raggruppa le componenti connesse per identificare le singole lettere, calcolando anche informazioni spaziali come distanze relative e disegnando le relative bounding box sull'immagine originale.
Ogni lettera viene poi ritagliata, ridimensionata proporzionalmente e centrata su un'immagine nera 28x28, rendendola pronta per le fasi successive. La classe inoltre fornisce metodi per accedere all'immagine segmentata, alle lettere preprocessate e alla loro visualizzazione.


\subsection*{PostProcessing}
La classe \texttt{ImageToStringPostprocessing} a partire dalla lista delle lettere classificate con relative informazioni spaziali, applica le euristiche discusse nei capitoli precedenti per decidere dove inserire spazi tra parole, basandosi sulle distanze orizzontali tra i caratteri. Inoltre, sfrutta la posizione verticale delle lettere rispetto al bounding box generale per correggere l'uso errato delle maiuscole e minuscole nei caratteri confondibili, confrontando ciascun carattere incerto con il primo considerato non confondibile.

\subsection*{Classificazione}
\texttt{ImageToStringClassifier} gestisce l'intero processo di riconoscimento: preprocessing, classificazione caratteri e postprocessing.

\section{Dataset}
La cartella \emph{dataset} contiene il notebook \texttt{dataset.ipynb}, in cui sono descritte e implementate tutte le procedure necessarie per la creazione dei dataset discussi nel Capitolo~\ref{ch:dataset}.

\subsection*{Dataset dei simboli singoli}
Nella prima parte del notebook vengono definite le funzioni per generare automaticamente immagini sintetiche di lettere, variando font e margini \ref{sec:single_dataset}.  
Segue una fase di normalizzazione delle immagini, in cui ciascuna immagine viene ridimensionata e centrata su uno sfondo uniforme di 28x28 pixel.

Ad ogni generazione, le informazioni relative all'immagine prodotta — come il percorso, la label di ground truth e le informazioni sui margini — vengono registrate all'interno del file \texttt{dataset.txt}.
A partire da questo file di testo, viene effettuata la suddivisione del dataset negli insiemi di training e test, rispettivamente pari al 75\% e al 25\%.

Il \texttt{DataLoader} farà riferimento a questi file per il caricamento dei dati.

\subsection*{Dataset degli screenshot}
Nella seconda parte è presente la funzione incaricata di generare il dataset degli screenshot \ref{sec:dataset_screenshots}, nelle sue due varianti. La generazione avviene analogamente al dataset dei singoli simboli, variando sempre gli stessi font.
Anche in questo caso, per ogni immagine generata viene prodotto un file di metadati compatibile con il \texttt{DataLoader}.

\section{Demo}
La cartella \emph{demo} contiene il file \texttt{demo.py}, che fornisce l'interfaccia per testare il modello descritta nel Capitolo \ref{cap:demo}. Lo script permette di caricare un'immagine, eseguire il preprocessing, la classificazione, il postprocessing e visualizzare il risultato finale.

\section{Src}
La cartella \emph{src} contiene i file principali per l'addestramento, la valutazione e l'esecuzione del modello. In particolare:
\begin{itemize}
    \item \texttt{main.ipynb}
    \item \texttt{ImageToStringNet.py}
    \item \texttt{evaluation\_english\_words.ipynb}
    \item \texttt{evaluation\_random\_words.ipynb}
\end{itemize}

\subsection*{main.ipynb}
Il notebook \texttt{main.ipynb} implementa l'intero workflow di addestramento e valutazione del modello.
All'inizio viene definita la classe \texttt{DigitDataset}, che funge da \texttt{DataLoader} per le fasi di training e testing.

Al fine di effettuare dei test durante il training, il dataset di train viene suddiviso in due sottoinsiemi: training e validation.
Viene quindi inizializzata la rete neurale definita nel file \texttt{ImageToStringNet.py}, insieme alla funzione di loss basata sulla \texttt{CrossEntropy} e all'ottimizzatore \texttt{SGD}.

Per l'addestramento, il flusso è diviso in varie fasi:
\begin{itemize}
    \item \textbf{Gestione configurazioni}
    \item \textbf{Logging Tensorboard}
    \item \textbf{Ciclo di addestramento e validation}
 \end{itemize}

\subsubsection*{Gestione configurazioni}
Le configurazioni da valutare vengono caricate da un file in formato \texttt{json} che specifica i parametri di addestramento da testare.  
Ogni configurazione definisce, il learning rate, il dropout rate e il valore di momentum. Per ciascuna configurazione viene istanziata una nuova rete neurale, inizializzata con il tasso di \texttt{dropout} specificato.
Successivamente, viene configurato un ottimizzatore \texttt{SGD} utilizzando i parametri indicati nella configurazione.
Nel caso in cui siano già disponibili pesi pre-addestrati per una determinata configurazione, questi vengono caricati automaticamente prima dell'inizio del training.

\subsubsection*{Logging Tensorboard}
Viene creato un writer che registra l'andamento della loss e dell'accuracy durante la fase di addestramento e validation per ogni epoca.

\subsubsection*{Ciclo di training e validation}

Per ogni epoca viene selezionato il \texttt{DataLoader} appropriato in base alla modalità corrente (training o validation).  
Il modello esegue la classificazione dei dati in input e calcola il valore della funzione di loss.  

Al termine di ciascun batch, i risultati vengono loggati tramite il writer.

Infine, al termine di ogni epoca, i pesi del modello vengono salvati seguendo uno schema di denominazione standard, che incorpora i principali iperparametri della configurazione corrente.

Le celle successive del notebook sono dedicate alla valutazione del modello, che viene effettuata solo dopo aver trovato i migliori parametri. la valuazione verrà approfondità nel capitolo~\ref{sec:valutazioni}.

\subsection*{ImageToStringNet.py}
Il file \texttt{ImageToStringNet.py} contiene la classe che implementa la rete neurale convoluzionale (CNN) utilizzata per la classificazione dei caratteri estratti, discussa nel Capitolo valutazioni~\ref{cap:modello}. 

Il costruttore della classe \texttt{ImageToStringNet} definisce l'architettura della rete, ed è suddiviso in due moduli principali:
\begin{itemize}
    \item \textbf{Feature Extractor}: implementa i due blocchi convoluzionali, dove ciascuno consiste di convoluzione, max pooling e ReLU.
    \item \textbf{Classifier}: prende in input le feature provenienti dal \texttt{feature\_extractor} e le elabora attraverso la rete di tre layer fully connected dove i primi due utilizzano dropout per prevenire l'overfitting seguite da ReLU e l'ultimo layer di output mappa gli 84 neuroni finali al numero di classi possibili.
\end{itemize}

Nel metodo \texttt{forward}, viene definito il flusso dell'input attraverso la rete, l'immagine viene processata dal modulo \texttt{feature$\_$extractor} restituendo un tensore che viene appiattito e vengono concatenati i due margini superiori e inferiori, infine viene passato al modulo \texttt{classifier} che produce l'output finale.

\subsection*{Notebook per Evaluation}
I notebook \texttt{evaluation\_english\_words.ipynb} ed \\  \texttt{evaluation\_random\_words.ipynb} sono dedicati alla valutazione delle prestazioni del modello sulle due varianti del dataset degli screenshot. 

In ogni notebook, per ciascuna immagine del dataset associato si esegue il riconoscimento del testo. I risultati ottenuti per ogni immagine vengono raccolti e usati per calcolare una serie di metriche approfondite nel capitolo di valutazione~\ref{sec:valutazioni}.