\chapter{Codice}
Il codice del progetto è suddiviso in quattro cartelle ciascuna delle quali si occupa di un aspetto specifico del flusso. Di seguito è riportata la struttura delle cartelle e dei file principali, per avere una panoramica dell'organizzazione del progetto. \\ 

\tikzstyle{every node}=[draw=black,thick,anchor=west]
\tikzstyle{selected}=[draw=red,fill=red!30]
\tikzstyle{optional}=[dashed,fill=gray!50]
\begin{tikzpicture}[%
  grow via three points={one child at (0.5,-0.68) and
  two children at (0.5,-0.71) and (0.5,-1.45)},
  edge from parent path={(\tikzparentnode.south) |- (\tikzchildnode.west)}]
  \node {root/}
    child { node {core/}
        child{ node {ImageToStringClassifier.py}}
        child{ node {ImageToStringPostprocessing.py}}
        child{ node {ImageToStringPreprocessing.py}}
    }
    child [missing] {}
    child [missing] {}
    child [missing] {}
    child { node {dataset/}
        child { node {dataset.ipynb}}
    }
    child [missing] {}
    child { node {demo/}
        child { node {demo.py}}
    }
    child [missing] {}
    child { node {src/}
        child { node {evaluation\_english\_words.ipynb}}
        child { node {evaluation\_random\_words.ipynb}}
        child { node {ImageToStringNet.py}}
        child { node {main.ipynb}}
    }
    child [missing] {}
    child [missing] {}
    child [missing] {}
    child [missing] {};
\end{tikzpicture}

\section{Core}
La cartella \emph{core} contiene il codice delle tre classi dedicate a preprocessing, classificazione e postprocessing. Ciascun file definisce una classe omonima.

\subsection{PreProcessing}
La classe \texttt{ImageToStringPreprocessing} prepara l'immagine contenente testo per la fase successiva di classificazione, segmentando le lettere e normalizzandole in un formato uniforme. A partire da un'immagine in input, esegue operazioni come conversione in scala di grigi, binarizzazione e inversione del contrasto se necessario. 
Successivamente, rileva e raggruppa le componenti connesse per identificare le singole lettere, calcolando anche informazioni spaziali come distanze relative e disegnando le relative bounding box sull'immagine originale.
Ogni lettera viene poi ritagliata, ridimensionata proporzionalmente e centrata su un'immagine nera 28x28, rendendola pronta per le fasi successive. La classe inoltre fornisce metodi per accedere all'immagine segmentata, alle lettere preprocessate e alla loro visualizzazione.

\subsection{Classificazione}
A partire da un'immagine contenente una sequenza di caratteri, la classe \texttt{ImageToStringClassifier} gestisce l'intero processo di riconoscimento integrando preprocessing, postprocessing e \texttt{ImageToStringNet} per la classificazione.

\subsection{PostProcessing}
La classe \texttt{ImageToStringPostprocessing} a partire dalla lista delle lettere classificate con relative informazioni spaziali, applica le euristiche discusse nei capitoli precedenti per decidere dove inserire spazi tra parole, basandosi sulle distanze orizzontali tra i caratteri. Inoltre, sfrutta la posizione verticale delle lettere rispetto ai bounding box generale per correggere l'uso errato delle maiuscole e minuscole in caratteri ambigui, confrontando ciascun carattere incerto con il primo considerato affidabile. Il risultato è una sequenza di caratteri più coerente, utile per migliorare l'output finale del sistema di OCR.

\section{Dataset}
La cartella \emph{dataset} contiene il notebook \texttt{dataset.ipynb}, in cui sono descritte e implementate tutte le procedure necessarie per la creazione dei dataset dei simboli singoli e degli screenshot.
\subsection{Dataset dei simboli singoli}
Nella prima parte del notebook, sono definite le funzioni per la generazione automatica delle immagini dei caratteri che permettono di creare immagini sintetiche di lettere, al variare di font e margini.
Segue una fase di normalizzazione delle immagini, in cui ciascuna immagine viene convertita in scala di grigi, ridimensionata e centrata su uno sfondo uniforme, 28x28 pixel. Il notebook include le procedure per la suddivisione del dataset in insiemi di training e test, distribuiti rispettivamente in 75\% e 25\%.
Viene definito il salvataggio dei dati in formato compatibile con PyTorch (\texttt{torch.utils.data.Dataset}) e come importarli rapidamente nei notebook di addestramento e valutazione.
\subsection{Dataset degli screenshot}
Nella parte finale del notebook è presente la funzione incaricata di generare il dataset degli screenshot, nelle sue due varianti. Una prima cella la richiama per generare immagini contenenti sequenze di 10 caratteri casuali, mentre una seconda cella la utilizza per generare frasi di senso compiuto, selezionate da un dataset di citazioni. In entrambi i casi, per ogni font presente nella lista fornita, vengono create 100 immagini con uno sfondo di un colore scelto casualmente. Per garantire una buona leggibilità del testo, viene calcolata la luminosità dello sfondo e, in base a essa, viene determinato se usare testo bianco o nero. Dopo aver centrato il testo all’interno dell’immagine, quest’ultima viene salvata all’interno della directory corrispondente al font, utilizzando un nome univoco generato automaticamente.
\section{Demo}
La cartella \emph{demo} contiene il file \texttt{demo.py}, che fornisce l'interfaccia semplice per testare il modello descritta nel Capitolo \ref{cap:demo}. Lo script permette di caricare un'immagine, eseguire il preprocessing, la classificazione e visualizzare il risultato finale.

\section{Src}
La cartella \emph{src} contiene i file principali per l'addestramento, la valutazione e l'esecuzione del modello. In particolare:
\begin{itemize}
    \item \texttt{main.ipynb}
    \item \texttt{ImageToStringNet.py}
    \item \texttt{evaluation\_english\_words.ipynb}
    \item \texttt{evaluation\_random\_words.ipynb}
\end{itemize}

\subsection{main.ipynb}
Il notebook \texttt{main.ipynb} implementa l'intero workflow di addestramento e valutazione del modello. Viene definita la classe \texttt{DigitDataset} che consente di gestire il dataset facilmente, dopo opportune trasformazioni, all'interno della pipeline di addestramento e valutazione in PyTorch.

Una volta impostata la dimensione del batch per l'addestramento, vengono definite le trasformazioni da applicare alle immagini ovvero conversione in tensori e normalizzazione. Viene suddisviso il dataset in training e test separando i font, garantendo che quelli utilizzati per il test non siano mai presenti nell'addestramento. I dati vengono poi caricati in memoria tramite i \texttt{DataLoarder} di PyTorch, che gestiscono automaticamente il batching e lo shuffle. Viene inizializzata la rete neurale implementata in \texttt{ImageToStringNet.py} e definita la funzione di loss e l'ottimizzatore SGD. Viene inoltre utilizzato un modulo di logger per TensorBoard, per monitorare visivamente un batch di immagini, salvare la struttura del modello, e le embedding delle immagini.

Per l'addestramento, il flusso è diviso in varie fasi:
\begin{itemize}
    \item \textbf{Gestione delle configurazioni:} per ogni configurazione viene avviata un'esecuzione indipendente dell'intero ciclo di addestramento e valutazione.
    \item \textbf{Costruzione e configurazione del modello:} per ogni configurazione viene istanziata una nuova rete con il tasso di dropout specificato. Viene poi configurato un ottimizzatore SGD con i parametri previsti.
    \item \textbf{Logging dei risultati con Tensorboard:} viene creato un writer che registra l'andamento della loss e dell'accuracy durante la fase di addestramento e valutazione e una visualizzazione grafica delle predizioni rispetto le etichette reali.
    \item \textbf{Caricamento dei pesi preesistenti:} se esistono dei pesi per una determinata configurazione, vengono caricati prima dell'addestramento.
    \item \textbf{Ciclo di addestramento ed epoche:} il modello viene addestrato per un numero definito di epoche.
    \item \textbf{Salvataggio dei pesi:} al termine di ogni configurazione, i pesi aggiornati vengono salvati.
 \end{itemize}
Le celle succesive del notebook sono dedicati alla valutazione del modello. Per ogni batch nel set di test, vengono calcolate e stampate la loss media, l'accuracy media, precision, recall e F1-score mantenendo la stessa importanza per tutte le classi.

Infine, vengono raccolti tutti i dati di test unendo tutti i batch, si fa una sola predizione su tutto il dataset, usando il modello e si crea un grafico che confronta le predizioni del modello con le etichette vere visualizzato in TensorBoard. 
\subsection{ImageToStringNet.py}
Il file \texttt{ImageToStringNet.py} contiene la classe che implementa la rete neurale convoluzionale (CNN) utilizzata per la classificazione dei caratteri estratti, discussa nel Capitolo~\ref{cap:modello}. 

Il costruttore della classe \texttt{ImageToStringNet} definisce l'architettura della rete, ed è suddiviso in due moduli principali:
\begin{itemize}
    \item \textbf{Feature Extractor}: implementa i due blocchi convoluzionali, dove ciascuno consiste di convoluzione, max pooling e ReLU.
    \item \textbf{Classifier}: prende in input le feature provenienti dal \texttt{feature\_extractor} e le elabora attraverso la rete di tre layer fully connected dove i primi due utilizzano dropout per prevenire l'overfitting seguite da ReLU e l'ultimo layer di output mappa gli 84 neuroni finali al numero di classi possibili.
\end{itemize}

Nel metodo \texttt{forward}, viene definito il flusso dell'input attraverso la rete, l'immagine viene processata dal modulo \texttt{feature$\_$extractor} restituendo un tensore che viene appiattito e vengono concatenati i due margini superiori e inferiori, infine viene passato al modulo \texttt{classifier} che produce l'output finale.

\subsection{Notebook per Evaluation}
I notebook \texttt{evaluation\_english\_words.ipynb} ed \\  \texttt{evaluation\_random\_words.ipynb} sono dedicati alla valutazione delle prestazioni del modello sulle due varianti del dataset degli screenshot. 

In ogni notebook, per ciascuna immagine del dataset associato si esegue il riconoscimento del testo. Il risultato viene confrontato con il testo atteso tramite la distanza di Levenshtein. I risultati ottenuti per ogni immagine vengono raccolti e usati per calcolare media, varianza, valore minimo e valore massimo. Per la valutazione dell'accuracy, viene definita: la precisione case-sensitive e case-insensitive per le lettere confondibili, versioni che ignorano gli spazi interni e le lettere come la "i" e la "l" maiuscole.


