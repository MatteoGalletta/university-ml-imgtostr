\chapter{Codice}
Il codice del progetto è suddiviso in quattro cartelle ciascuna delle quali si occupa di un aspetto specifico del flusso. Di seguito è riportata la struttura delle cartelle e dei file principali, per avere una panoramica dell'organizzazione del progetto. \\ 

\tikzstyle{every node}=[draw=black,thick,anchor=west]
\tikzstyle{selected}=[draw=red,fill=red!30]
\tikzstyle{optional}=[dashed,fill=gray!50]
\begin{tikzpicture}[%
  grow via three points={one child at (0.5,-0.68) and
  two children at (0.5,-0.71) and (0.5,-1.45)},
  edge from parent path={(\tikzparentnode.south) |- (\tikzchildnode.west)}]
  \node {root/}
    child { node {core/}
        child{ node {ImageToStringClassifier.py}}
        child{ node {ImageToStringPostprocessing.py}}
        child{ node {ImageToStringPreprocessing.py}}
    }
    child [missing] {}
    child [missing] {}
    child [missing] {}
    child { node {dataset/}
        child { node {dataset.ipynb}}
    }
    child [missing] {}
    child { node {demo/}
        child { node {demo.py}}
    }
    child [missing] {}
    child { node {src/}
        child { node {evaluation\_english\_words.ipynb}}
        child { node {evaluation\_random\_words.ipynb}}
        child { node {ImageToStringNet.py}}
        child { node {main.ipynb}}
    }
    child [missing] {}
    child [missing] {}
    child [missing] {}
    child [missing] {};
\end{tikzpicture}

\section{Core}
La cartella \emph{core} contiene il codice delle tre classi dedicate a preprocessing, classificazione e postprocessing. Ciascun file definisce una classe omonima.

\subsection{PreProcessing}
La classe \texttt{ImageToStringPreprocessing} prepara l'immagine contenente testo per la fase successiva di classificazione, segmentando le lettere e normalizzandole in un formato uniforme. A partire da un'immagine in input, esegue operazioni come conversione in scala di grigi, binarizzazione e inversione del contrasto se necessario. 
Successivamente, rileva e raggruppa le componenti connesse per identificare le singole lettere, calcolando anche informazioni spaziali come distanze relative e disegnando le relative bounding box sull'immagine originale.
Ogni lettera viene poi ritagliata, ridimensionata proporzionalmente e centrata su un'immagine nera 28x28, rendendola pronta per le fasi successive. La classe inoltre fornisce metodi per accedere all'immagine segmentata, alle lettere preprocessate e alla loro visualizzazione.

\subsection{Classificazione}
A partire da un'immagine contenente una sequenza di caratteri, la classe \texttt{ImageToStringClassifier} gestisce l'intero processo di riconoscimento integrando preprocessing, postprocessing e \texttt{ImageToStringNet} per la classificazione.

\subsection{PostProcessing}
La classe \texttt{ImageToStringPostprocessing} a partire dalla lista delle lettere classificate con relative informazioni spaziali, applica le euristiche discusse nei capitoli precedenti per decidere dove inserire spazi tra parole, basandosi sulle distanze orizzontali tra i caratteri. Inoltre, sfrutta la posizione verticale delle lettere rispetto ai bounding box generale per correggere l'uso errato delle maiuscole e minuscole in caratteri ambigui, confrontando ciascun carattere incerto con il primo considerato affidabile. Il risultato è una sequenza di caratteri più coerente, utile per migliorare l'output finale del sistema di OCR.

\section{Dataset}
La cartella \emph{dataset} contiene il notebook \texttt{dataset.ipynb}, in cui sono descritte e implementate tutte le procedure necessarie per la creazione dei dataset dei simboli singoli e degli screenshot.
\subsection{Dataset dei simboli singoli}
Nella prima parte del notebook, sono definite le funzioni per la generazione automatica delle immagini dei caratteri che permettono di creare immagini sintetiche di lettere, al variare di font e margini.
Segue una fase di normalizzazione delle immagini, in cui ciascuna immagine viene convertita in scala di grigi, ridimensionata e centrata su uno sfondo uniforme, 28x28 pixel. Il notebook include le procedure per la suddivisione del dataset in insiemi di training e test, distribuiti rispettivamente in 75\% e 25\%.
Viene definito il salvataggio dei dati in formato compatibile con PyTorch (\texttt{torch.utils.data.Dataset}) e come importarli rapidamente nei notebook di addestramento e valutazione.
\subsection{Dataset degli screenshot}
Nella parte finale del notebook è presente la funzione incaricata di generare il dataset degli screenshot, nelle sue due varianti. Una prima cella la richiama per generare immagini contenenti sequenze di 10 caratteri casuali, mentre una seconda cella la utilizza per generare frasi di senso compiuto, selezionate da un dataset di citazioni. In entrambi i casi, per ogni font presente nella lista fornita, vengono create 100 immagini con uno sfondo di un colore scelto casualmente. Per garantire una buona leggibilità del testo, viene calcolata la luminosità dello sfondo e, in base a essa, viene determinato se usare testo bianco o nero. Dopo aver centrato il testo all’interno dell’immagine, quest’ultima viene salvata all’interno della directory corrispondente al font, utilizzando un nome univoco generato automaticamente.
\section{Demo}
La cartella \emph{demo} contiene il file \texttt{demo.py}, che fornisce l'interfaccia semplice per testare il modello descritta nel Capitolo \ref{cap:demo}. Lo script permette di caricare un'immagine, eseguire il preprocessing, la classificazione e visualizzare il risultato finale.

\section{Src}
La cartella \emph{src} contiene i file principali per l'addestramento, la valutazione e l'esecuzione del modello. In particolare:
\begin{itemize}
    \item \texttt{main.ipynb}
    \item \texttt{ImageToStringNet.py}
    \item \texttt{evaluation\_english\_words.ipynb}
    \item \texttt{evaluation\_random\_words.ipynb}
\end{itemize}

\subsection{main.ipynb}
Il notebook \texttt{main.ipynb} implementa l'intero workflow di addestramento e valutazione del modello. La pipeline inizia definendo la classe \texttt{DigitDataset} con l'obiettivo di caricare le immagini dei caratteri, le relative etichette e le informazioni spaziali di margine superiore e inferiore.

\subsection{ImageToStringNet.py}
Il file \texttt{ImageToStringNet.py} contiene la classe che implementa la rete neurale convoluzionale (CNN) utilizzata per la classificazione dei caratteri estratti, discussa nel Capitolo~\ref{cap:modello}. 

Il costruttore della classe \texttt{ImageToStringNet} definisce l'architettura della rete, ed è suddiviso in due moduli principali:
\begin{itemize}
    \item \textbf{Feature Extractor}: implementa i due blocchi convoluzionali, dove ciascuno consiste di convoluzione, max pooling e ReLU.
    \item \textbf{Classifier}: prende in input le feature provenienti dal \texttt{feature\_extractor} e le elabora attraverso la rete di tre layer fully connected dove i primi due utilizzano dropout per prevenire l'overfitting seguite da ReLU e l'ultimo layer di output mappa gli 84 neuroni finali al numero di classi possibili.
\end{itemize}

Nel metodo \texttt{forward}, viene definito il flusso dell'input attraverso la rete, l'immagine viene processata dal modulo \texttt{feature$\_$extractor} restituendo un tensore che viene appiattito e vengono concatenati i due margini superiori e inferiori, infine viene passato al modulo \texttt{classifier} che produce l'output finale.

\subsection{Notebook per Evaluation}
I notebook \texttt{evaluation\_english\_words.ipynb} ed \\  \texttt{evaluation\_random\_words.ipynb} sono dedicati alla valutazione delle prestazioni del modello sulle due varianti del dataset degli screenshot. 

In ogni notebook, per ciascuna immagine del dataset associato si esegue il riconoscimento del testo. Il risultato viene confrontato con il testo atteso tramite la distanza di Levenshtein. I risultati ottenuti per ogni immagine vengono raccolti e usati per calcolare media, varianza, valore minimo e valore massimo. Per la valutazione dell'accuracy, viene definita: la precisione case-sensitive e case-insensitive per le lettere confondibili, versioni che ignorano gli spazi interni e le lettere come la "i" e la "l" maiuscole.


